---
title: Hacker News Scraper
---

In this tutorial we are going to write a scraper to extract information from hacker
news pages.

This scraper will be capable of:

- Listing hacker news pages: news, past, ask
- Read user data
- Read comments from items
- Navigate on pagination

## Attribute mapping

The first step is to make names for every attribute we want to deal with.

Here is a quick guide you can follow to help in this process:

1. Pick a general prefix, usually the product/service name, for this case I'll pick: `hacker-news`.
2. For data points, add some entity context in the name. In the case of Hacker News I see they call it `item`.
3. If you mean to read collections, give each collection a name.

Let's see what are the interesting data points to extract from the Hacker News front
page:

<div className="pathom-diagram">

  ![Front Page Mapping](../assets/tutorials/hacker-news-scraper/front-page-mapping.png)

</div>

The circle cursors point to visible points of data, the open diamond means the data
is hidden (inside the markup).

I used the name `:hacker-news.page/news` to name the collection of items for this page.

Here is a text version of all the declared attributes:

```clojure
; item attributes
:hacker-news.item/age
:hacker-news.item/author-name
:hacker-news.item/id
:hacker-news.item/comments-count
:hacker-news.item/score
:hacker-news.item/rank-in-page
:hacker-news.item/source
:hacker-news.item/title
:hacker-news.item/url

; news collection
:hacker-news.page/news
```

## Reading the news page

For the implementation I'll use the [Hickory](https://github.com/davidsantiago/hickory) library to parse the HTML and extract
the data.

To start, we need to explore and figure the code to extract the information fragments
from the HTML page.

I like to start reading the raw html and saving on a `defonce`, so I can keep reloading
the REPL while have a cached version of sample data:

```clojure
(ns com.wsscode.pathom3.docs.demos.tutorials.hacker-news-scrapper)

(defonce sample-html (slurp (str "https://news.ycombinator.com/news")))
```

It's time to learn about the HTML structure of Hacker News page, I like to use the
Chrome inspector to navigate. I can see there is a table with the class `itemlist`
wrapping the item elements.

<div className="pathom-diagram">

  ![Front Page Mapping](../assets/tutorials/hacker-news-scraper/inspect-container.png)

</div>

### Finding the items

I'll start this query using our data and Hickory and try it on the REPL, I suggest
you follow along in your REPL:

```clojure
(ns com.wsscode.pathom3.docs.demos.tutorials.hacker-news-scrapper
  (:require [hickory.core :as hc]
            [hickory.select :as hs]))

(defonce sample-html (slurp (str "https://news.ycombinator.com/news")))

(comment
  ; navigate to table element
  (->> sample-html
       (hc/parse)
       (hc/as-hickory)
       (hs/select (hs/class "itemlist"))
       first))
```

Now we can extract the rows out. Hacker News doesn't make it strait forward. When I
look at the rows I see each item uses two table rows. Then it has a spacer row between
the next two with the class `spacer`. To add more details, in the end there is a
different row with the class `morespace`.

To deal with this, we are going to query for rows, removing the ones with the class `spacer`
or `morespace`.

This is how we can do it with Hickory:

```clojure
(comment
  (->> sample-html
       (hc/parse)
       (hc/as-hickory)
       (hs/select (hs/class "itemlist"))
       first
       (hs/select (hs/and
                    (hs/tag "tr")
                    (hs/not (hs/or
                              (hs/class "spacer")
                              (hs/class "morespace")))))
       (partition 2)
       (mapv #(hash-map :type :element :tag :tbody :content (vec %)))))
```

Now we have collection where each item represent a row in Hacker News.

:::note
After the `partition` the `mapv` is making a fake element for Hickory, this way we
can threat the two rows as a single element for querying in.
:::

### Read item details

Now for each item we need to extract the attributes we want. Here are a few helpers
we will use for it:

```clojure
(defn find-text
  "Given an element, traverse the contents until it reaches some text."
  [el]
  (loop [item (first (:content el))]
    (if (string? item)
      item
      (if-let [next (some-> item :content first)]
        (recur next)
        nil))))

(defn class-text
  "Get the text for a given element that matches a css class."
  [el class]
  (->> (hs/select (hs/class class) el)
       first
       (find-text)))

(defn select-number
  "Extract the first integer from a string."
  [x]
  (if-let [[_ n] (re-find #"(\d+)" (str x))]
    (Integer/parseInt n)
    0))
```

Here is a function to extract the data points we mentioned at start from each item:

```clojure
(defn extract-item-from-hickory [el]
  {:hacker-news.item/age            (class-text el "age")
   :hacker-news.item/author-name    (class-text el "hnuser")
   :hacker-news.item/comments-count (->> (hs/select (hs/find-in-text #"comments$") el)
                                         first
                                         (find-text)
                                         (select-number))
   :hacker-news.item/score          (select-number (class-text el "score"))
   :hacker-news.item/id             (->> el :content first :attrs :id)
   :hacker-news.item/rank-in-page   (select-number (class-text el "rank"))
   :hacker-news.item/source         (class-text el "sitestr")
   :hacker-news.item/title          (class-text el "storylink")
   :hacker-news.item/url            (->> (hs/select (hs/class "storylink") el)
                                         first :attrs :href)})
```

Let's use it to extract the data from our previous process:

```clojure
(comment
  (->> sample-html
       (hc/parse)
       (hc/as-hickory)
       (hs/select (hs/class "itemlist"))
       first
       (hs/select (hs/and
                    (hs/tag "tr")
                    (hs/not (hs/or
                              (hs/class "spacer")
                              (hs/class "morespace")))))
       (partition 2)
       (mapv #(hash-map :type :element :tag :tbody :content (vec %)))
       (mapv extract-item-from-hickory)))
```

Now you should see the plain nice plain data in the output, with one map for each
item entry.

## Make a resolver

Time to introduce Pathom, now I'm going to turn that exploration code in a resolver:

```clojure {2-4,47-87}
(ns com.wsscode.pathom3.docs.demos.tutorials.hacker-news-scrapper
  (:require [com.wsscode.pathom3.connect.indexes :as pci]
            [com.wsscode.pathom3.connect.operation :as pco]
            [com.wsscode.pathom3.interface.eql :as p.eql]
            [hickory.core :as hc]
            [hickory.select :as hs]))

(defn find-text
  "Given an element, traverse the contents until it reaches some text."
  [el]
  (loop [item (first (:content el))]
    (if (string? item)
      item
      (if-let [next (some-> item :content first)]
        (recur next)
        nil))))

(defn class-text
  "Get the text for a given element that matches a css class."
  [el class]
  (->> (hs/select (hs/class class) el)
       first
       (find-text)))

(defn select-number
  "Extract the first integer from a string."
  [x]
  (if-let [[_ n] (re-find #"(\d+)" (str x))]
    (Integer/parseInt n)
    0))

(defn extract-item-from-hickory [el]
  {:hacker-news.item/age            (class-text el "age")
   :hacker-news.item/author-name    (class-text el "hnuser")
   :hacker-news.item/comments-count (->> (hs/select (hs/find-in-text #"comments$") el)
                                         first
                                         (find-text)
                                         (select-number))
   :hacker-news.item/score          (select-number (class-text el "score"))
   :hacker-news.item/id             (->> el :content first :attrs :id)
   :hacker-news.item/rank-in-page   (select-number (class-text el "rank"))
   :hacker-news.item/source         (class-text el "sitestr")
   :hacker-news.item/title          (class-text el "storylink")
   :hacker-news.item/url            (->> (hs/select (hs/class "storylink") el)
                                         first :attrs :href)})

(pco/defresolver news-page-html-string []
  {:hacker-news.page/news-raw-html
   (slurp "https://news.ycombinator.com/news")})

(pco/defresolver news-page [{:hacker-news.page/keys [news-raw-html]}]
  {::pco/output
   [{:hacker-news.page/news
     [:hacker-news.item/age
      :hacker-news.item/author-name
      :hacker-news.item/id
      :hacker-news.item/comments-count
      :hacker-news.item/score
      :hacker-news.item/rank-in-page
      :hacker-news.item/source
      :hacker-news.item/title
      :hacker-news.item/url]}]}
  {:hacker-news.page/news
   (->> news-raw-html
        (hc/parse)
        (hc/as-hickory)
        (hs/select (hs/class "itemlist"))
        first
        (hs/select (hs/and
                     (hs/tag "tr")
                     (hs/not (hs/or
                               (hs/class "spacer")
                               (hs/class "morespace")))))
        (partition 2)
        (mapv #(hash-map :type :element :tag :tbody :content (vec %)))
        (mapv extract-item-from-hickory))})

(def env
  (pci/register
    [news-page-html-string
     news-page]))

(comment
  ; get the title of all the news
  (p.eql/process env
    [{:hacker-news.page/news
      [:hacker-news.item/title]}]))
```

Not much yet, but we gained the ability to filter pieces of the results.

:::tip
Some editors like [Cursive](https://cursive-ide.com/) do highlight keywords when your
cursor is over them. You can use this indication to see the inputs connecting with
outputs in the editor.
:::

### Caching the request for development

When we run the process now, it does the request to hacker news every time. For development
its useful if we can cache that to have a faster iteration.

You may have noticed I used a separated resolver to fetch the HTML string. We can make
this resolver use a durable cache to speed up the iteration:

```clojure
(pco/defresolver news-page-html-string []
  ; define a custom cache store for this resolver
  {::pco/cache-store ::durable-cache*}
  {:hacker-news.page/news-raw-html
   (slurp "https://news.ycombinator.com/news")})

; defonce to have a durable cache, an atom with a map is a valid empty cache
(defonce cache* (atom {}))

(def env
  ; add our cache to the environment
  (-> {::durable-cache* cache*}
      (pci/register
        [news-page-html-string
         news-page])))
```

Now if you keep running that expression, it only does the IO on the first time.

:::tip
To clear the cache, run `(reset! cache* {})` in the REPL.
:::

## Navigate the history

So far we only read the first page of the news, but there is a `More` button at the
end. Now we are going to scan though these pages.

First we need to adapt the resolver that loads the page html to accept a custom URL
for it:

```clojure
(pco/defresolver news-page-html-string [{:hacker-news.page/keys [news-page-url]}]
  {::pco/cache-store ::durable-cache*
   ::pco/input       [(pco/? :hacker-news.page/news-page-url)]}
  {:hacker-news.page/news-raw-html
   (slurp (or news-page-url "https://news.ycombinator.com/news"))})
```

I used an [optional input](resolvers.mdx#optional-inputs) named `:hacker-news.page/news-page-url` to allow the
customization, but still have a default.

To provide the data with the URL for the next page, I'll add a new resolver. This resolver
will expose the attribute `:hacker-news.page/news-next-page` that contains the key
`:hacker-news.page/news-page-url`:

```clojure
(pco/defresolver news-next-page [{:hacker-news.page/keys [news-raw-html]}]
  {::pco/output
   [{:hacker-news.page/news-next-page
     [:hacker-news.page/news-page-url]}]}
  (let [link (some->> news-raw-html hc/parse hc/as-hickory
               (hs/select (hs/class "morelink"))
               first :attrs :href)]
    (if link
      {:hacker-news.page/news-next-page
       {:hacker-news.page/news-page-url
        (str "https://news.ycombinator.com/" link)}})))
```

I check if there is a `More` link, otherwise we don't return any data, to tell Pathom
this is unavailable.

You may notice we now have two resolvers that get the HTML string for the news page
and parse it. Each resolver is doing its own parsing, we can make then share this
by breaking this step in a new resolver.

```clojure
; get the html string and compute hickory
(pco/defresolver news-page-hickory [{:hacker-news.page/keys [news-raw-html]}]
  {:hacker-news.page/news-hickory
   (-> news-raw-html
       (hc/parse)
       (hc/as-hickory))})

; news page now uses the hickory
(pco/defresolver news-page [{:hacker-news.page/keys [news-hickory]}]
  {::pco/output
   [{:hacker-news.page/news
     [:hacker-news.item/age
      :hacker-news.item/author-name
      :hacker-news.item/id
      :hacker-news.item/comments-count
      :hacker-news.item/score
      :hacker-news.item/rank-in-page
      :hacker-news.item/source
      :hacker-news.item/title
      :hacker-news.item/url]}]}
  {:hacker-news.page/news
   (->> news-hickory
        (hs/select (hs/class "itemlist"))
        first
        (hs/select (hs/and
                     (hs/tag "tr")
                     (hs/not (hs/or
                               (hs/class "spacer")
                               (hs/class "morespace")))))
        (partition 2)
        (mapv #(hash-map :type :element :tag :tbody :content (vec %)))
        (mapv extract-item-from-hickory))})

; same for next page
(pco/defresolver news-next-page [{:hacker-news.page/keys [news-hickory]}]
  {::pco/output
   [{:hacker-news.page/news-next-page
     [:hacker-news.page/news-page-url]}]}

  (let [link (some->> news-hickory
               (hs/select (hs/class "morelink"))
               first :attrs :href)]
    (if link
      {:hacker-news.page/news-next-page
       {:hacker-news.page/news-page-url
        (str "https://news.ycombinator.com/" link)}})))
```

This is an important design choice when you write Pathom resolvers. How much you want
to break, as you add more resolvers you expand the connection points to other resolvers.

In general is a good practice to keep spread, but it's also fine to provide many items
in a resolver when they share close process. This reduces the amount of work Pathom
has to do to integrate.

Let's play with our new resolvers:

```clojure {6,8}
; remember to update env to include all resolvers
(def env
  (-> {::durable-cache* cache*}
      (pci/register
        [news-page-html-string
         news-page-hickory
         news-page
         news-next-page])))

(comment
  ; get titles from first and second page
  (p.eql/process env
    [{:hacker-news.page/news
      [:hacker-news.item/title]}
     {:hacker-news.page/news-next-page
      [{:hacker-news.page/news
        [:hacker-news.item/title]}]}]))
```

Notice the query inside `:hacker-news.page/news-next-page` is the same used in the
parent query. For this we can use recursive queries, let's say we want to pull the
next three pages:

```clojure
(comment
  (p.eql/process env
    [{:hacker-news.page/news
      [:hacker-news.item/title]}
     ; recurse bounded to 3 steps
     {:hacker-news.page/news-next-page 3}]))
```

How cool is that?! You may be saying now: ok, but that's a weird tree output.

To flat the items out we can use `tree-seq`:

```clojure
(comment
  (->> (p.eql/process env
         [{:hacker-news.page/news
           [:hacker-news.item/title]}
          ; recurse bounded to 3 steps
          {:hacker-news.page/news-next-page 3}])
       (tree-seq :hacker-news.page/news-next-page
         ; we need vector at the end because tree-seq expects children to be a collection
         (comp vector :hacker-news.page/news-next-page))
       ; mapcat the news to have a single flat list
       (into [] (mapcat :hacker-news.page/news))))
```

:::note
Recursive queries can be numbers (bounded) or a symbol ... (unbounded). If you use the
unbounded it's going to pull pages until Hacker News is over with them. During the time
I tested there were 21 pages. If you try, it may take some time to finish.
:::

Pathom also supports [nested inputs](resolvers.mdx#nested-inputs), this means we can create a resolver to make that
same process we did with the query before:

```clojure
(pco/defresolver all-news-pages [input]
  {::pco/input  [{:hacker-news.page/news
                  [:hacker-news.item/age
                   :hacker-news.item/author-name
                   :hacker-news.item/id
                   :hacker-news.item/comments-count
                   :hacker-news.item/score
                   :hacker-news.item/rank-in-page
                   :hacker-news.item/source
                   :hacker-news.item/title
                   :hacker-news.item/url]}
                 ; note the recursive query here
                 {:hacker-news.page/news-next-page '...}]
   ::pco/output [{:hacker-news.page/news-all-pages
                  [:hacker-news.item/age
                   :hacker-news.item/author-name
                   :hacker-news.item/id
                   :hacker-news.item/comments-count
                   :hacker-news.item/score
                   :hacker-news.item/rank-in-page
                   :hacker-news.item/source
                   :hacker-news.item/title
                   :hacker-news.item/url]}]}
  {:hacker-news.page/news-all-pages
   (->> input
        (tree-seq :hacker-news.page/news-next-page
          (comp vector :hacker-news.page/news-next-page))
        (into [] (mapcat :hacker-news.page/news)))})
```

Now we can, for example, make this query to read all titles in news, in all pages:

```clojure
(comment
  ; this can take a while
  (->> (p.eql/process env
         [{:hacker-news.page/news-all-pages
           [:hacker-news.item/title]}])))
```

## Read user data

Let's look at the user page this time:

<div className="pathom-diagram">

  ![User Page Mapping](../assets/tutorials/hacker-news-scraper/user-page-mapping.png)

</div>

Similar to before, but this time we require some user id to load the page. The arrows
show that the same attribute we read on the page as `:hacker-news.user/id` is used
in the URL to load the page.

Here are the resolvers to parse this:

```clojure
(pco/defresolver user-data-hickory [{:keys [hacker-news.user/id]}]
  ; also use the durable cache here
  {::pco/cache-store ::durable-cache*}
  {:hacker-news.page/user-hickory
   (some-> (slurp (str "https://news.ycombinator.com/user?id=" id))
     hc/parse hc/as-hickory)})

(pco/defresolver user-data [{:hacker-news.page/keys [user-hickory]}]
  {:hacker-news.user/karma
   (->> user-hickory
        (hs/select
          (hs/and
            (hs/tag "tr")
            (hs/has-child (hs/find-in-text #"karma:"))))
        first :content second :content select-number)

   :hacker-news.user/join-date
   (let [str (->> user-hickory
                  (hs/select
                    (hs/and
                      (hs/tag "tr")
                      (hs/has-child (hs/find-in-text #"created:"))))
                  first :content second :content first :attrs :href)
         [_ date] (re-find #"(\d{4}-\d{2}-\d{2})" str)]
     date)})
```

Note we also use the durable cache, so we can keep playing it. When I created this I
was just hitting the same cache entry until I got the extraction code right.

:::important
You may have notice that we now have two different attributes that mean user id.
We have `:hacker-news.item/author-name` and now `:hacker-news.user/id`. This means
if we try to load the karma for the user in the HN item, it won't be able to get there.

One idea is to change our previous resolver and rename `:hacker-news.item/author-name`
to `:hacker-news.user/id`. This would work, but this reduces the accuracy of this name
semantics. `:hacker-news.item/author-name` have a very specific meaning, it's the author
name in an item.

To reconcile this situation we can create an [alias-resolver](built-in-resolvers.mdx#aliasing),
that allows Pathom from navigate from one name to another. This is what I'm going to
use next.

It's also good to point out that aliases are directional. We are allowing
`:hacker-news.item/author-name` to be translated in `:hacker-news.user/id`, but not
the reverse.
:::

Let's see who has the most karma from the front-page:

```clojure
; update env
(def env
  (-> {::durable-cache* cache*}
      (pci/register
        [news-page-html-string
         news-page-hickory
         news-page
         news-next-page
         user-data-hickory
         user-data
         ; alias the attributes
         (pbir/alias-resolver :hacker-news.item/author-name :hacker-news.user/id)])))

(comment
  (->> (p.eql/process env
         [{:hacker-news.page/news
           [:hacker-news.item/author-name
            :hacker-news.user/karma]}])
       :hacker-news.page/news
       (sort-by :hacker-news.user/karma #(compare %2 %))))
```

## Read comments

Let's map the comments section:

<div className="pathom-diagram">

  ![Comments Page Mapping](../assets/tutorials/hacker-news-scraper/comments-page-mapping.png)

</div>

We can see at the top we have almost the same data as we did on the news list, except
the rank position (which makes sense, since its relative to that page).

Let's start writing a resolver that can read this information given some item id:

```clojure
; load page as hickory, use our durable cache
(pco/defresolver item-page-hickory [{:hacker-news.item/keys [id]}]
  {::pco/cache-store ::durable-cache*}
  {:hacker-news.page/item-hickory
   (->> (slurp (str "https://news.ycombinator.com/item?id=" id))
        hc/parse hc/as-hickory)})


(pco/defresolver item-data [{:hacker-news.page/keys [item-hickory]}]
  {::pco/output
   [:hacker-news.item/age
    :hacker-news.item/author-name
    :hacker-news.item/comments-count
    :hacker-news.item/score
    :hacker-news.item/source
    :hacker-news.item/title
    :hacker-news.item/url]}
  ; let's re-use our same extraction function, fatitem class is the container at
  ; this page
  (extract-item-from-hickory (->> (hs/select (hs/class "fatitem") item-hickory)
                                  first)))
```

That's easy enough, time for the comments.

By looking at the page we can see comments are nested, and altough this example isn't
showing, they also support deep nesting.

Most of the time, the HTML will follow the structure of the data, but this isn't the
case here.

Try inspecting the page, you will see they use a flat table, and manually add the
spacings to convey the nesting.

For us this means we need to do more work to reconstruct the tree from a flat structure.

Let's do that in parts, first lets extract it in a way that is closer to what we have,
that is a flat structure.

```clojure

```

## Other pages

## Recursive nested inputs, scanning it all
